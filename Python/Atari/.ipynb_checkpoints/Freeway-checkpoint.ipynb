{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, img_height, img_width):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Just a few simple hidden layers\n",
    "        self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24)\n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=3)\n",
    "        \n",
    "    # will implement a forward pass to the network\n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    # This capacity is the only parameter that needs to be specified when creating a replay memory object\n",
    "    # memory [] will be the structure that actually holds the stored experiences\n",
    "    # push_count is how many experiences we've added to memory\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "    \n",
    "    def push(self, experience):\n",
    "        # accepts an experience. We have to first check that the memory is less than capacity\n",
    "        # otherwise we begin to push experiences on to the front of memory. Overriding the oldest \n",
    "        # experience first \n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    \n",
    "    # These sampled experiences are what we are going to use to train the DQN\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    \n",
    "    # Returns the calculated exploration rate \n",
    "    # Our agent is going to then use this exploration rate to determine how it should select its actions\n",
    "    # either by exploring or exploiting the environment\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    # num_actions is the number of possible actions an agent can take from a given state\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        # corresponds to the current step number in the environment\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "    \n",
    "    # policy_net is from DQN\n",
    "    def select_action(self, state, policy_net):\n",
    "        # exploration rate returned from the epsilon greedy strategy that was passed in when we created our agent\n",
    "        rate = strategy.get_exploration_rate(self.current_step)\n",
    "        # increment the agents current step by 1\n",
    "        self.current_step += 1\n",
    "    \n",
    "        # check to see if the exploration rate is greater than a randomly generated number between 0 and 1\n",
    "        if rate > random.random():\n",
    "            # Then we explore the environment by randomly selecting an action, 0, 1, or 2\n",
    "            return torch.tensor([[random.randrange(self.num_actions)]], device=device, dtype=torch.long)\n",
    "        else:\n",
    "            # Otherwise we exploit the environment by selecting the action that corresponds to the highest\n",
    "            # q value output from our policy network for the given state\n",
    "            # no_grad will turn off gradient tracking since we are currently using the model just for inference\n",
    "            # and not for training\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).max(1)[1].view(1, 1) # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreewayManager():\n",
    "    def __init__(self, device, movie):\n",
    "        self.device = device\n",
    "#         self.env = Monitor(gym.make('Freeway-v0').unwrapped, movie, force=True)\n",
    "        self.env = gym.make('Freeway-v0').unwrapped\n",
    "        self.env.reset()\n",
    "        # will keep track of the screen at any given time. None means we are at the start\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "    \n",
    "    # returns an initial observation of the environment\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "    \n",
    "    # returns the number of actions available to an agent in the environment\n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    # requires an action to be passed in\n",
    "    # calls step on the environment\n",
    "    # we only care about the reward and whether or not it is done\n",
    "    # The action that will be passed to our main function will be a tensor\n",
    "    # Item just returns the value of this tensor as a standard python number which is what step expects\n",
    "    # Returns the reward wrapped in pytorch tensor. So we have a tensor coming in and a tensor coming out\n",
    "    def take_action(self, action):\n",
    "        state, reward, self.done, _ = self.env.step(action.item())\n",
    "        if action.item() == 1:\n",
    "            reward += 0.01\n",
    "        \n",
    "#         if action.item() == 2:\n",
    "#             reward -= 0.1\n",
    "        \n",
    "        return torch.tensor([reward], device=self.device)\n",
    "\n",
    "    \n",
    "    # returns true if the current screen is none\n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    # return the current state of the environment in the form of a processed image of the screen\n",
    "    # we represent a single state in the environment as the difference between the current screen \n",
    "    # and the previous screen\n",
    "    # the first screen before you start is represented by a black screen and the screen that corresponds\n",
    "    # to when the episode ends is also going to be a black screen \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "    \n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render('rgb_array').transpose((2, 0, 1))\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[1]\n",
    "        \n",
    "        # Strip off any of the screen not needed\n",
    "        top = int(screen_height * 0.07)\n",
    "        bottom = int(screen_height * 0.93)\n",
    "        screen = screen[:, top:bottom, :]\n",
    "        return screen\n",
    "    \n",
    "    def transform_screen_data(self, screen):\n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        # stored sequentially next to eachother in memory\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        # used to chain together several image transformations\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((80,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # unsqueeze represents a batch dimension since the processed images will\n",
    "        # be passed to the DQN in batches\n",
    "        return resize(screen).unsqueeze(0).to(self.device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period, episode, rewards):\n",
    "    plt.figure(2)\n",
    "    plt.clf\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('# Of Times Crossing Highway')\n",
    "    plt.ylabel('Crossing Time')\n",
    "    plt.plot(values)\n",
    "    \n",
    "    moving_avg = get_moving_average(moving_avg_period, values) \n",
    "    plt.plot(moving_avg)\n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode:\", episode, \"\\n\", \\\n",
    "         \"Score:\", moving_avg_period, \" and Rewards:\", rewards, \"\\naverage crossing time:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "        \n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    # we can't calculate a moving average of a data set when the data set is not at least as large\n",
    "    # as the period we wan't to calculate the moving average for\n",
    "    if len(values) >= period:\n",
    "        # returns a tensor containing all slices with a size equal to the period that was passed in\n",
    "        # it does this on the 0th dimension of the original values tensor \n",
    "        # This gives us a new tensor containing all slices of size X across the original value tensor\n",
    "        # We then take the mean of each of the slizes and flatten the tensor so that now the moving average is \n",
    "        # equal to a tensor containing all X period moving averages from the values that were passed in\n",
    "        # We then concatenate this resulting tensor to a tensor of 0s with a size equal to period - 1\n",
    "        # This is to show that the moving average for the first period - 1 values is 0 \n",
    "        # Then convert the moving average tensor to a numpy array and return the result\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        # A numpy of all zeros with a length equal to the values array that was passed in\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# The discount factor used in the bellman equation\n",
    "gamma = 0.999\n",
    "# Starting value of epsilon\n",
    "eps_start = 1\n",
    "# Ending value of epsilon\n",
    "eps_end = 0.01\n",
    "# The decay rate used to decay epsilon over time\n",
    "eps_decay = 0.001\n",
    "# How frequently, in terms of episodes, to update the networks weights with the policy networks weights\n",
    "target_update = 10\n",
    "# The capacity of the replay memory\n",
    "memory_size = 100000\n",
    "# learning rate that is used during training of the policy net\n",
    "lr = 0.001\n",
    "# The number of episodes to play\n",
    "num_episodes = 1000\n",
    "\n",
    "# Use the gpu if it is available otherwise cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# em -> Environment Manager\n",
    "mov = './atari/freeway/optimizer/_0'\n",
    "em = FreewayManager(device, mov)\n",
    "# An instance of the epsilon greedy strategy class\n",
    "# Pass in the start, end, and decay values for epsilon\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "# Define an agent using the agent class and pass in the required strategy, number of actions available, and device\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "# instance of replaymemory with capacity of memory_size\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "# Set the weights and targets in the target net to be the same as those in the policy net using PyTorches\n",
    "# state_dict and load_state_dict functions \n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "# Put the target net into eval mode which tells PyTorch that this network is not in training mode \n",
    "# This network will only be used for inference \n",
    "target_net.eval()\n",
    "# Adam optimizer accepts are policy_net parameters as those for which we will be optimizing \n",
    "# and our defined learning rate \n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "# ready to start training \n",
    "prev_avg = 1000\n",
    "episode_times = []\n",
    "for episode in range(num_episodes):\n",
    "    # for each episode we first reset the environment and then get the inital state\n",
    "    scored = 0\n",
    "    total_rewards = 0\n",
    "    prev_timesteps = 0\n",
    "#     best_time = 10000\n",
    "    worst_time = 0\n",
    "    em.env.reset()\n",
    "    em.current_screen = None\n",
    "    \n",
    "    state = em.get_state()\n",
    "    \n",
    "    for timestep in count():\n",
    "        # action is selected based on the current state\n",
    "        # the agent will be using the policy_net network to select its action \n",
    "        # if it exploits the environment rather than explores it \n",
    "        em.env.render()\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        # Create and ecperience and store it in replay memory and set state to the next state\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        sleep(0.01)\n",
    "        \n",
    "        if reward.item() > 1:\n",
    "            scored += 1\n",
    "            \n",
    "            if (timestep - prev_timesteps) > prev_avg:\n",
    "#                 print(\"lost\")\n",
    "                reward -= 1\n",
    "                \n",
    "            episode_times.append((timestep - prev_timesteps))\n",
    "            prev_timesteps = timestep\n",
    "            \n",
    "        total_rewards += reward.item()\n",
    "        \n",
    "        # Now that the agent has had an experience and stored it in replay memory we now check to \n",
    "        # see if we can get a sample from replay memory to train our policy net\n",
    "        # We can get a sample equal to the batch size from replay memory as long as\n",
    "        # the current size of memory is at least the batch size\n",
    "        \n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            # get a sample equal to batch_size\n",
    "            experiences = memory.sample(batch_size)\n",
    "            # extract into their own tensors\n",
    "            batch = Experience(*zip(*experiences))\n",
    "            \n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, \\\n",
    "                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "            \n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "            states = torch.cat(batch.state)\n",
    "            actions = torch.cat(batch.action)\n",
    "            rewards = torch.cat(batch.reward)\n",
    "            next_states = torch.cat(batch.next_state)\n",
    "            \n",
    "            # get the Q values for the corresponding state action pairs that we've \n",
    "            # extracted from our experiences in batch\n",
    "            current_q_values = policy_net(states).gather(1, actions)\n",
    "    \n",
    "            next_q_values = torch.zeros(batch_size, device=device)\n",
    "            next_q_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "            # Can calculate the target Q values by multiplying each of the next q values by our discount\n",
    "            # rate gamma and add this result to the corresponding reward in the rewards tensor to create\n",
    "            # a new tensor of target q values\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "            \n",
    "            # We now can calculate the loss between the current q values and the target q values\n",
    "            # using mean squared error as our loss function and then we zero out the gradients\n",
    "            # using optimizer. Sets the gradients of all the weights and biases in the policy net to 0 \n",
    "            # Since PyTorch accumulates the gradients when it does back prop, we need to call zero grad\n",
    "            # before back prop occurs. Otherwise if we didn't zero out the gradients each time then we would be \n",
    "            # accumulating gradients across all backprop runs\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            # Copmutes the gradient of the loss with respect to all the weights and biases in the policy net\n",
    "            loss.backward()\n",
    "            # Updates the weights and biases with the gradients that we computed when we called backward on\n",
    "            # our loss\n",
    "            optimizer.step()\n",
    "        \n",
    "        if em.done:\n",
    "            plot(episode_times, scored, episode+1, total_rewards)\n",
    "            prev_avg = timestep / scored + 20\n",
    "#             print(prev_avg)\n",
    "            break\n",
    "    \n",
    "    # updating every 10 episodes\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
